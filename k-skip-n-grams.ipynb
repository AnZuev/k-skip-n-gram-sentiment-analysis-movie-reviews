{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_CORES = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for getting k-skip-n-grams from the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k - words between, if k = 0 then we get just n-grams\n",
    "# n - number of words in tuple, n = 2 -> bigram\n",
    "# also all k_skip_n_grams are sorted inside the tuple in order to get rid of duplicates\n",
    "def get_k_skip_n_grams(review, k_value=0, n_value=1, unique=False):\n",
    "    unsorted_k_skip_n_grams = zip(*[review[i*(k_value+1):] for i in range(n_value)])\n",
    "    sorted_k_skip_n_grams = [tuple(sorted(k_skip_n_gram)) for k_skip_n_gram in unsorted_k_skip_n_grams]\n",
    "    if unique:\n",
    "        return set(sorted_k_skip_n_grams)\n",
    "    else:\n",
    "        return sorted_k_skip_n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for creating dictionary of k_skip_n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import itertools\n",
    "import math\n",
    "TH = 0\n",
    "\n",
    "\n",
    "# returns a Counter object: \n",
    "# key - k_skip_n_gram\n",
    "# values - counter of k_skip_n_gram\n",
    "def get_counter_for_k_skip_n_grams(reviews, k_value=0, n_value=2, unique=False):\n",
    "    k_skip_n_grams_lists = POOL.map(functools.partial(get_k_skip_n_grams, k_value=k_value, n_value=n_value, unique=unique), reviews)\n",
    "    all_k_skip_n_grams = Counter(itertools.chain(*k_skip_n_grams_lists))\n",
    "    return all_k_skip_n_grams\n",
    "\n",
    "def get_prediction(inp, self):\n",
    "        index, review = inp\n",
    "        k_skip_n_grams = get_k_skip_n_grams(review, k_value=self.k, n_value=self.n)\n",
    "\n",
    "\n",
    "        positive_probability = math.log10(functools.reduce(operator.mul, \n",
    "                                                map(lambda k_skip_n_gram: self.positive[k_skip_n_gram] if self.positive[k_skip_n_gram] > 0 else 1, k_skip_n_grams),\n",
    "                                                1))\n",
    "        negative_probability = math.log10(functools.reduce(operator.mul, \n",
    "                                                map(lambda k_skip_n_gram: self.negative[k_skip_n_gram] if self.negative[k_skip_n_gram] > 0 else 1, k_skip_n_grams),\n",
    "                                                1))\n",
    "        if positive_probability/self._positive_len - TH >= negative_probability/self._negative_len:\n",
    "            return (index, 1)\n",
    "        else:\n",
    "            return (index, 0)\n",
    "\n",
    "def get_proba_prediction(inp, self):\n",
    "    index, review = inp\n",
    "    k_skip_n_grams = get_k_skip_n_grams(review, k_value=self.k, n_value=self.n)\n",
    "\n",
    "    positive_probability = math.log10(functools.reduce(operator.mul, \n",
    "                                                map(lambda k_skip_n_gram: self.positive[k_skip_n_gram] if self.positive[k_skip_n_gram] > 0 else 1, k_skip_n_grams),\n",
    "                                                1))\n",
    "    negative_probability = math.log10(functools.reduce(operator.mul, \n",
    "                                                map(lambda k_skip_n_gram: self.negative[k_skip_n_gram] if self.negative[k_skip_n_gram] > 0 else 1, k_skip_n_grams),\n",
    "                                                1))\n",
    "    return (index, positive_probability/self._positive_len, negative_probability/self._negative_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class K_skip_n_gram_probability_model():\n",
    "    def __init__(self, n_value=2, k_value=1):\n",
    "        self.k = k_value\n",
    "        self.n = n_value\n",
    "        self.positive = Counter()\n",
    "        self.negative = Counter()\n",
    "        self._positive_reviews = []\n",
    "        self._negative_reviews = []\n",
    "        self._positive_len = 0\n",
    "        self._negative_len = 0\n",
    "    \n",
    "    # self optimisation of the model\n",
    "    def optimise(self, x_train, y_train, x_test, y_test):\n",
    "        k_values = [0, 1, 2, 3, 4, 5, 6]\n",
    "        n_values = [1, 2, 3]\n",
    "        \n",
    "        # all possible cases of pair (n, k)\n",
    "        cases = [(x, y) for x in n_values for y in k_values]\n",
    "        print(cases)\n",
    "        n = self.n\n",
    "        k = self.k\n",
    "        # list of tuples:\n",
    "        # tuple[0]: case\n",
    "        # tuple[1]: accuracy_train\n",
    "        # tuple[2]: accuracy_test\n",
    "        result = [] \n",
    "        \n",
    "        # for all values n, k fit the model and get accuracy\n",
    "        for case in cases:\n",
    "            # case[0] - n\n",
    "            # case[1] - k\n",
    "            self.n = int(case[0])\n",
    "            self.k = int(case[1])\n",
    "            self.internal_fit()\n",
    "            train_accuracy = accuracy_score(y_train, self.predict(x_train))\n",
    "            test_accuracy = accuracy_score(y_test, self.predict(x_test))\n",
    "            result.append((case, train_accuracy, test_accuracy))\n",
    "            print(\"For case: k =\", case[1], \"n =\", case[0], \"train_acc =\", train_accuracy, \" test acc =\", test_accuracy)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def predict(self, reviews):\n",
    "        result = map(functools.partial(get_prediction, self=self), enumerate(reviews))\n",
    "        result = sorted(result, key=lambda x: x[0], reverse=False)\n",
    "        return np.array(list(map(lambda x: x[1], result)))\n",
    "            \n",
    "    def predict_proba(self, reviews):\n",
    "        result = map(functools.partial(get_proba_prediction, self=self), enumerate(reviews))\n",
    "        result = sorted(result, key=lambda x: x[0], reverse=False)\n",
    "        return np.array(list(map(lambda x: (x[1], x[2]), result)))\n",
    "    \n",
    "    # reviews is also used to initialise/update dictionary of n_grams\n",
    "    def fit(self, reviews, labels):\n",
    "        self._positive_reviews = reviews[labels == 1]\n",
    "        self._negative_reviews = reviews[labels == 0]\n",
    "        self.positive = get_counter_for_k_skip_n_grams(self._positive_reviews, k_value=self.k, n_value=self.n)\n",
    "        self.negative = get_counter_for_k_skip_n_grams(self._negative_reviews, k_value=self.k, n_value=self.n)\n",
    "        self._positive_len = float(sum(self.positive.values()))\n",
    "        self._negative_len = float(sum(self.negative.values()))\n",
    "    \n",
    "    def internal_fit(self):\n",
    "        self.positive = get_counter_for_k_skip_n_grams(self._positive_reviews, k_value=self.k, n_value=self.n)\n",
    "        self.negative = get_counter_for_k_skip_n_grams(self._negative_reviews, k_value=self.k, n_value=self.n)\n",
    "        self._positive_len = float(sum(self.positive.values()))\n",
    "        self._negative_len = float(sum(self.negative.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check our model\n",
    "Use k_value = 0 and n_value = 2 (ordinary bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'POOL' in globals():\n",
    "    POOL.close()\n",
    "POOL = multiprocessing.Pool(NUMBER_OF_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)\n",
    "max_review_length = 150\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = K_skip_n_gram_probability_model(n_value=3, k_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97596\n",
      "0.75768\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_train, model.predict(X_train)))\n",
    "print(accuracy_score(y_test, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get good accuracy on training and but not so for for the test one <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to get more!\n",
    "\n",
    "I wrote a method that tries to fit the model and to compute accuracy for different values of *n* and *k*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6)]\n",
      "For case: k = 0 n = 1 train_acc = 0.83068  test acc = 0.80288\n",
      "For case: k = 1 n = 1 train_acc = 0.83068  test acc = 0.80288\n",
      "For case: k = 2 n = 1 train_acc = 0.83068  test acc = 0.80288\n",
      "For case: k = 3 n = 1 train_acc = 0.83068  test acc = 0.80288\n",
      "For case: k = 4 n = 1 train_acc = 0.83068  test acc = 0.80288\n",
      "For case: k = 5 n = 1 train_acc = 0.83068  test acc = 0.80288\n",
      "For case: k = 6 n = 1 train_acc = 0.83068  test acc = 0.80288\n",
      "For case: k = 0 n = 2 train_acc = 0.96056  test acc = 0.85624\n",
      "For case: k = 1 n = 2 train_acc = 0.96544  test acc = 0.83216\n",
      "For case: k = 2 n = 2 train_acc = 0.96512  test acc = 0.80872\n",
      "For case: k = 3 n = 2 train_acc = 0.9636  test acc = 0.79052\n",
      "For case: k = 4 n = 2 train_acc = 0.96192  test acc = 0.78256\n",
      "For case: k = 5 n = 2 train_acc = 0.96264  test acc = 0.77808\n",
      "For case: k = 6 n = 2 train_acc = 0.95984  test acc = 0.77212\n",
      "For case: k = 0 n = 3 train_acc = 0.98232  test acc = 0.8458\n",
      "For case: k = 1 n = 3 train_acc = 0.97596  test acc = 0.75768\n",
      "For case: k = 2 n = 3 train_acc = 0.96884  test acc = 0.69908\n",
      "For case: k = 3 n = 3 train_acc = 0.96792  test acc = 0.67056\n",
      "For case: k = 4 n = 3 train_acc = 0.96548  test acc = 0.66496\n",
      "For case: k = 5 n = 3 train_acc = 0.96468  test acc = 0.65872\n",
      "For case: k = 6 n = 3 train_acc = 0.96396  test acc = 0.65744\n"
     ]
    }
   ],
   "source": [
    "model.optimise(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best result for training data 0.98232 (n = 3, k = 0) and it corresponds to 0.8458 for the test data <br>\n",
    "But the best result for test data is 0.85624 (n = 2, k = 0). <br>\n",
    "Also there is one more interesting case (n = 2, k = 1), accuracy for training data is 0.96544 and 0.83216 for the test. This result is close to (n = 3, k = 0) but here we use 1-skip-2-gram. May be it is a good idea to combine these 2 kind of features? (but not now, firtsly let's try to remove num_words limit while loading imdb data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to remove num_words\n",
    "May be we use very few of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "max_review_length = 150\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95476\n",
      "0.62396\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "res = model.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_train, model.predict(X_train)))\n",
    "print(accuracy_score(y_test, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, not so good. <br>\n",
    "Perform the same trick as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6)]\n",
      "For case: k = 0 n = 1 train_acc = 0.8904  test acc = 0.8064\n",
      "For case: k = 1 n = 1 train_acc = 0.8904  test acc = 0.8064\n",
      "For case: k = 2 n = 1 train_acc = 0.8904  test acc = 0.8064\n",
      "For case: k = 3 n = 1 train_acc = 0.8904  test acc = 0.8064\n",
      "For case: k = 4 n = 1 train_acc = 0.8904  test acc = 0.8064\n",
      "For case: k = 5 n = 1 train_acc = 0.8904  test acc = 0.8064\n",
      "For case: k = 6 n = 1 train_acc = 0.8904  test acc = 0.8064\n",
      "For case: k = 0 n = 2 train_acc = 0.97416  test acc = 0.86204\n",
      "For case: k = 1 n = 2 train_acc = 0.97596  test acc = 0.83564\n",
      "For case: k = 2 n = 2 train_acc = 0.97376  test acc = 0.80588\n",
      "For case: k = 3 n = 2 train_acc = 0.97328  test acc = 0.78788\n",
      "For case: k = 4 n = 2 train_acc = 0.97124  test acc = 0.77688\n",
      "For case: k = 5 n = 2 train_acc = 0.97184  test acc = 0.7698\n",
      "For case: k = 6 n = 2 train_acc = 0.9708  test acc = 0.76344\n",
      "For case: k = 0 n = 3 train_acc = 0.9808  test acc = 0.8466\n",
      "For case: k = 1 n = 3 train_acc = 0.96972  test acc = 0.74756\n",
      "For case: k = 2 n = 3 train_acc = 0.96168  test acc = 0.6738\n",
      "For case: k = 3 n = 3 train_acc = 0.95876  test acc = 0.6432\n",
      "For case: k = 4 n = 3 train_acc = 0.95656  test acc = 0.632\n",
      "For case: k = 5 n = 3 train_acc = 0.95616  test acc = 0.62616\n",
      "For case: k = 6 n = 3 train_acc = 0.95476  test acc = 0.62396\n"
     ]
    }
   ],
   "source": [
    "model.optimise(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is no (n, k) with result for training data more than 0.98232. It seems that not very popular words add some noise to our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to combine (n = 3, k = 0 with n = 2, k = 1)\n",
    "At the very beginning analyse do the result that we get using one case (n = 3, k = 0) differs from (n=2, k=1). If not then there is no sense to combine them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check n=3, k =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_k_0_n_3 = K_skip_n_gram_probability_model(n_value=3, k_value=0)\n",
    "model_k_0_n_3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proba_k_0_n_3 = model_k_0_n_3.predict_proba(X_train)\n",
    "pred_k_0_n_3 = model_k_0_n_3.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9808\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_train, pred_k_0_n_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check n=2, k =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_k_1_n_2 = K_skip_n_gram_probability_model(n_value=2, k_value=1)\n",
    "model_k_1_n_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proba_k_1_n_2 = model_k_1_n_2.predict_proba(X_train)\n",
    "pred_k_1_n_2 = model_k_1_n_2.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97596\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_train, pred_k_1_n_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How much differents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differs 457 out of 25000\n"
     ]
    }
   ],
   "source": [
    "differs = abs(pred_k_0_n_3 - pred_k_1_n_2)\n",
    "print(\"Differs\", sum(differs), \"out of\", len(pred_k_0_n_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it differs in about 2.54% cases <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0, n=3: 480 out of 25000\n",
      "k=1, n=2: 601 out of 25000\n"
     ]
    }
   ],
   "source": [
    "differs_from_correct_k_0_n_3 = abs(pred_k_0_n_3 - y_train)\n",
    "differs_from_correct_k_1_n_2 = abs(pred_k_1_n_2 - y_train)\n",
    "\n",
    "print(\"k=0, n=3:\", sum(differs_from_correct_k_0_n_3), \"out of 25000\\nk=1, n=2:\", sum(differs_from_correct_k_1_n_2), \"out of 25000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k = 0, n = 3\n",
      "TN = 12106, FP = 394, FN = 86, TP = 12414\n",
      "For k = 1, n = 2\n",
      "TN = 12085, FP = 415, FN = 186, TP = 12314\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, pred_k_0_n_3).ravel()\n",
    "print(\"For k = 0, n = 3\")\n",
    "print(\"TN = {}, FP = {}, FN = {}, TP = {}\".format(tn, fp, fn, tp))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, pred_k_1_n_2).ravel()\n",
    "print(\"For k = 1, n = 2\")\n",
    "print(\"TN = {}, FP = {}, FN = {}, TP = {}\".format(tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.41057068  0.35839217]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.0590531 ,  3.04103   ],\n",
       "       [ 3.73373458,  3.71342058],\n",
       "       [ 0.76666234,  0.76861977],\n",
       "       [ 2.75635218,  2.74798172],\n",
       "       [ 3.05455717,  3.02927125],\n",
       "       [ 1.61980208,  1.61946917],\n",
       "       [ 0.31621605,  0.31674948],\n",
       "       [ 0.38754928,  0.39404225],\n",
       "       [ 2.98942058,  2.975877  ],\n",
       "       [ 3.14943938,  3.13018762],\n",
       "       [ 2.66365471,  2.65665944],\n",
       "       [ 3.17378825,  3.17228542],\n",
       "       [ 1.19691187,  1.19293327],\n",
       "       [ 2.40962981,  2.40793736],\n",
       "       [ 0.46165212,  0.52007053],\n",
       "       [ 2.04593629,  2.07522349],\n",
       "       [ 3.05452479,  3.0523358 ],\n",
       "       [ 3.12990524,  3.11663693],\n",
       "       [ 2.4473232 ,  2.41795186],\n",
       "       [ 1.5109398 ,  1.48862741],\n",
       "       [ 0.21698496,  0.2188403 ],\n",
       "       [ 3.1558286 ,  3.13190802],\n",
       "       [ 2.86043501,  2.86037296],\n",
       "       [ 2.48072072,  2.45697993],\n",
       "       [ 2.94170211,  2.89968909],\n",
       "       [ 3.24011942,  3.23334443],\n",
       "       [ 2.42784392,  2.37735821],\n",
       "       [ 0.33587618,  0.38743197],\n",
       "       [ 1.98657573,  1.9780771 ],\n",
       "       [ 2.67251694,  2.6472031 ],\n",
       "       [ 1.90135417,  1.8934789 ],\n",
       "       [ 2.4293861 ,  2.41693006],\n",
       "       [ 2.132828  ,  2.1099666 ],\n",
       "       [ 2.55849192,  2.53350555],\n",
       "       [ 0.39753538,  0.43179807],\n",
       "       [ 2.76648704,  2.75336936],\n",
       "       [ 1.88064804,  1.87072075],\n",
       "       [ 0.22266318,  0.22481999],\n",
       "       [ 2.85368977,  2.83183359],\n",
       "       [ 3.23320949,  3.204243  ],\n",
       "       [ 2.56821773,  2.56489297],\n",
       "       [ 2.97582936,  2.95904997],\n",
       "       [ 0.40866059,  0.46392247],\n",
       "       [ 0.43556401,  0.44684358],\n",
       "       [ 3.48908144,  3.48358957],\n",
       "       [ 3.01153911,  2.98823005],\n",
       "       [ 1.97564556,  1.96973407],\n",
       "       [ 3.15825419,  3.15337558],\n",
       "       [ 1.02673939,  1.02900953],\n",
       "       [ 3.02481749,  3.00493695],\n",
       "       [ 1.01218666,  1.00503922],\n",
       "       [ 2.94530537,  2.93221894],\n",
       "       [ 2.10871372,  2.0926076 ],\n",
       "       [ 1.75656049,  1.72335055],\n",
       "       [ 0.32714868,  0.31107007],\n",
       "       [ 0.91638942,  0.91366036],\n",
       "       [ 0.31473765,  0.31554288],\n",
       "       [ 3.07556136,  3.05050304],\n",
       "       [ 3.05655549,  3.0446909 ],\n",
       "       [ 1.48590643,  1.48341096],\n",
       "       [ 3.31091547,  3.27792669],\n",
       "       [ 2.7784376 ,  2.75968843],\n",
       "       [ 1.52451088,  1.50695878],\n",
       "       [ 0.61716228,  0.60890128],\n",
       "       [ 2.4599332 ,  2.44928   ],\n",
       "       [ 2.43194525,  2.41400105],\n",
       "       [ 1.31189948,  1.29050555],\n",
       "       [ 2.32610528,  2.31782797],\n",
       "       [ 2.63024503,  2.60986471],\n",
       "       [ 3.41925522,  3.39781483],\n",
       "       [ 0.23568256,  0.24017341],\n",
       "       [ 0.35849001,  0.35965757],\n",
       "       [ 2.80191624,  2.79253069],\n",
       "       [ 0.35139871,  0.35086824],\n",
       "       [ 2.38923565,  2.37132861],\n",
       "       [ 0.97075482,  0.94853672],\n",
       "       [ 0.132097  ,  0.13177607],\n",
       "       [ 0.35247567,  0.35581435],\n",
       "       [ 2.46410271,  2.4545083 ],\n",
       "       [ 0.42954726,  0.43106945],\n",
       "       [ 2.5005611 ,  2.49791353],\n",
       "       [ 0.3433199 ,  0.34332222],\n",
       "       [ 0.36336463,  0.36654758],\n",
       "       [ 2.62676741,  2.62622901],\n",
       "       [ 3.73188389,  3.72930903],\n",
       "       [ 0.42166386,  0.46248148],\n",
       "       [ 2.73566202,  2.71731595],\n",
       "       [ 2.28428463,  2.27906353],\n",
       "       [ 2.3970183 ,  2.39288423],\n",
       "       [ 1.07834024,  1.07596863],\n",
       "       [ 1.76382363,  1.75709233],\n",
       "       [ 1.64883216,  1.64373356],\n",
       "       [ 2.76614994,  2.75347863],\n",
       "       [ 3.08673011,  3.07902611],\n",
       "       [ 2.14279016,  2.13880976],\n",
       "       [ 3.05116985,  3.02363393],\n",
       "       [ 3.2714882 ,  3.26269992],\n",
       "       [ 1.99879081,  1.99328367],\n",
       "       [ 3.02866256,  2.94695605],\n",
       "       [ 1.11480007,  1.11861482],\n",
       "       [ 2.82537168,  2.81512642],\n",
       "       [ 3.52267993,  3.49839665],\n",
       "       [ 3.12799439,  3.10982542],\n",
       "       [ 1.01692915,  1.01709389],\n",
       "       [ 0.87092173,  0.82619994],\n",
       "       [ 0.3261186 ,  0.34305193],\n",
       "       [ 2.44079681,  2.4324912 ],\n",
       "       [ 0.32952908,  0.34010824],\n",
       "       [ 2.03714812,  2.01997339],\n",
       "       [ 2.28251362,  2.2601031 ],\n",
       "       [ 0.46756299,  0.47121582],\n",
       "       [ 1.14545504,  1.14275731],\n",
       "       [ 3.27868482,  3.2412887 ],\n",
       "       [ 3.00605759,  2.997376  ],\n",
       "       [ 2.37307852,  2.35146356],\n",
       "       [ 2.19588227,  2.19222235],\n",
       "       [ 0.27924987,  0.28575976],\n",
       "       [ 0.51103999,  0.51935353],\n",
       "       [ 2.91406108,  2.90689   ],\n",
       "       [ 2.90005082,  2.88288159],\n",
       "       [ 0.24555009,  0.2351954 ],\n",
       "       [ 2.22258318,  2.21807651],\n",
       "       [ 1.25822063,  1.24778456],\n",
       "       [ 3.2032159 ,  3.1592806 ],\n",
       "       [ 3.05978389,  3.04528745],\n",
       "       [ 3.35945317,  3.32597732],\n",
       "       [ 1.80507172,  1.8009574 ],\n",
       "       [ 3.00775939,  3.0028201 ],\n",
       "       [ 1.47053128,  1.45568937],\n",
       "       [ 0.25702451,  0.25132803],\n",
       "       [ 3.94982163,  3.90904567],\n",
       "       [ 2.85208035,  2.83154116],\n",
       "       [ 3.00873999,  3.00304213],\n",
       "       [ 1.35935077,  1.35393295],\n",
       "       [ 3.25224451,  3.24727633],\n",
       "       [ 2.86697553,  2.84056306],\n",
       "       [ 0.20096248,  0.19394979],\n",
       "       [ 3.50513659,  3.476736  ],\n",
       "       [ 2.61634229,  2.60285951],\n",
       "       [ 3.99224717,  3.96160186],\n",
       "       [ 3.01318752,  2.98256213],\n",
       "       [ 0.16366537,  0.16452238],\n",
       "       [ 3.21600124,  3.21403976],\n",
       "       [ 1.73107515,  1.71975323],\n",
       "       [ 3.57359599,  3.53780106],\n",
       "       [ 3.62920681,  3.59660899],\n",
       "       [ 2.77414424,  2.75159123],\n",
       "       [ 2.85775809,  2.84671884],\n",
       "       [ 2.98014223,  2.9485084 ],\n",
       "       [ 0.19888466,  0.20705969],\n",
       "       [ 3.10305096,  3.0788717 ],\n",
       "       [ 2.93133607,  2.92234887],\n",
       "       [ 2.73117717,  2.72630778],\n",
       "       [ 2.5866701 ,  2.56402441],\n",
       "       [ 2.96953467,  2.95287701],\n",
       "       [ 2.53189101,  2.52661185],\n",
       "       [ 0.36877944,  0.36974969],\n",
       "       [ 0.16681096,  0.1647559 ],\n",
       "       [ 3.08148139,  3.06311225],\n",
       "       [ 2.73401382,  2.71442062],\n",
       "       [ 2.77589354,  2.74668905],\n",
       "       [ 0.31252602,  0.29007083],\n",
       "       [ 2.87797908,  2.87392527],\n",
       "       [ 3.17553606,  3.15472247],\n",
       "       [ 2.07812234,  2.0767085 ],\n",
       "       [ 2.67398289,  2.66473549],\n",
       "       [ 2.43093225,  2.42851915],\n",
       "       [ 2.05220702,  2.05185997],\n",
       "       [ 3.31558951,  3.30396813],\n",
       "       [ 0.26265731,  0.24271833],\n",
       "       [ 0.92862986,  0.92800414],\n",
       "       [ 2.31157434,  2.29621416],\n",
       "       [ 0.41134639,  0.39543304],\n",
       "       [ 3.18585313,  3.18415658],\n",
       "       [ 3.10704881,  3.10291716],\n",
       "       [ 0.26574027,  0.27921436],\n",
       "       [ 2.25129904,  2.20609169],\n",
       "       [ 2.19774808,  2.18788979],\n",
       "       [ 1.01330268,  1.00833828],\n",
       "       [ 2.79619844,  2.78717157],\n",
       "       [ 3.00667991,  2.99396248],\n",
       "       [ 1.01967509,  0.9925654 ],\n",
       "       [ 3.13866244,  3.11007944],\n",
       "       [ 2.88570548,  2.85939938],\n",
       "       [ 2.66893899,  2.64232895],\n",
       "       [ 0.17281009,  0.16645315],\n",
       "       [ 0.21901583,  0.21163397],\n",
       "       [ 2.63813161,  2.63311369],\n",
       "       [ 2.64738875,  2.62947325],\n",
       "       [ 3.26134093,  3.24604763],\n",
       "       [ 2.84081045,  2.81541202],\n",
       "       [ 0.35163562,  0.39710904],\n",
       "       [ 3.25876647,  3.22250577],\n",
       "       [ 2.43592576,  2.42299867],\n",
       "       [ 1.02306616,  1.02814843],\n",
       "       [ 0.2743488 ,  0.25785388],\n",
       "       [ 1.1454738 ,  1.16817793],\n",
       "       [ 0.31444943,  0.30127193],\n",
       "       [ 0.28920686,  0.27577136],\n",
       "       [ 3.04170042,  3.01845102],\n",
       "       [ 2.54101401,  2.51294414],\n",
       "       [ 1.23399303,  1.27793987],\n",
       "       [ 1.46255624,  1.44470065],\n",
       "       [ 0.15574802,  0.1628325 ],\n",
       "       [ 2.79874419,  2.79734197],\n",
       "       [ 0.38447459,  0.3969744 ],\n",
       "       [ 0.40149707,  0.40233773],\n",
       "       [ 2.70864808,  2.70387483],\n",
       "       [ 2.96315949,  2.9614947 ],\n",
       "       [ 0.46943799,  0.48181854],\n",
       "       [ 2.83289806,  2.81382312],\n",
       "       [ 1.05433782,  1.02656347],\n",
       "       [ 2.69509968,  2.6716727 ],\n",
       "       [ 2.80416818,  2.77893281],\n",
       "       [ 0.37408911,  0.38779512],\n",
       "       [ 2.69485074,  2.6920022 ],\n",
       "       [ 2.80421858,  2.77579517],\n",
       "       [ 3.47121515,  3.46204101],\n",
       "       [ 2.72176044,  2.71794816],\n",
       "       [ 2.80529789,  2.79892224],\n",
       "       [ 0.26120602,  0.26218705],\n",
       "       [ 0.23945712,  0.24195894],\n",
       "       [ 2.12721304,  2.12022342],\n",
       "       [ 3.37021242,  3.34296418],\n",
       "       [ 0.14987505,  0.14262945],\n",
       "       [ 0.26747684,  0.28001455],\n",
       "       [ 0.31211278,  0.30843174],\n",
       "       [ 2.93609445,  2.91761675],\n",
       "       [ 2.64667236,  2.62103813],\n",
       "       [ 2.91028879,  2.9031725 ],\n",
       "       [ 2.41527449,  2.41349894],\n",
       "       [ 2.20696732,  2.20397695],\n",
       "       [ 1.94779451,  1.94179517],\n",
       "       [ 1.17593865,  1.21069217],\n",
       "       [ 2.91951253,  2.90459209],\n",
       "       [ 0.20523702,  0.20353325],\n",
       "       [ 0.35896653,  0.38267825],\n",
       "       [ 1.54466303,  1.58641932],\n",
       "       [ 1.81872409,  1.80770572],\n",
       "       [ 0.4447685 ,  0.48752017],\n",
       "       [ 0.26608358,  0.26602867],\n",
       "       [ 3.0765277 ,  3.07259035],\n",
       "       [ 0.33967018,  0.34254255],\n",
       "       [ 3.13353093,  3.10929388],\n",
       "       [ 0.19166467,  0.20150691],\n",
       "       [ 2.19419139,  2.17202836],\n",
       "       [ 0.37615519,  0.39267834],\n",
       "       [ 1.04114056,  1.03902101],\n",
       "       [ 3.92561261,  3.88253369],\n",
       "       [ 2.58164514,  2.58076464],\n",
       "       [ 2.72904723,  2.69870463],\n",
       "       [ 2.63298846,  2.62967306],\n",
       "       [ 3.83580863,  3.79450945],\n",
       "       [ 0.31514146,  0.32201856],\n",
       "       [ 2.41992917,  2.38816037],\n",
       "       [ 2.02985972,  2.00874867],\n",
       "       [ 0.14277829,  0.1413787 ],\n",
       "       [ 0.38086219,  0.39611895],\n",
       "       [ 0.2645922 ,  0.25575379],\n",
       "       [ 2.43750182,  2.43556754],\n",
       "       [ 2.5524946 ,  2.53149164],\n",
       "       [ 3.04118976,  3.0088665 ],\n",
       "       [ 2.71709059,  2.70277477],\n",
       "       [ 3.42778193,  3.41076668],\n",
       "       [ 2.91062258,  2.90670413],\n",
       "       [ 2.6468704 ,  2.63263101],\n",
       "       [ 3.87744661,  3.83431204],\n",
       "       [ 2.21794544,  2.20777245],\n",
       "       [ 0.25600647,  0.25070484],\n",
       "       [ 1.32715005,  1.30539752],\n",
       "       [ 3.01195732,  2.99984876],\n",
       "       [ 2.72256842,  2.71504248],\n",
       "       [ 0.33616618,  0.3508132 ],\n",
       "       [ 2.84111061,  2.82881984],\n",
       "       [ 0.32602991,  0.32400781],\n",
       "       [ 1.51843098,  1.5173302 ],\n",
       "       [ 3.20127185,  3.17601472],\n",
       "       [ 2.81336856,  2.80666701],\n",
       "       [ 2.98005289,  2.96077737],\n",
       "       [ 2.4424402 ,  2.42163527],\n",
       "       [ 2.81530247,  2.80961692],\n",
       "       [ 3.21001517,  3.19832488],\n",
       "       [ 2.9641029 ,  2.94758414],\n",
       "       [ 3.14348174,  3.14331821],\n",
       "       [ 0.85748105,  0.84852559],\n",
       "       [ 2.79210739,  2.78743904],\n",
       "       [ 2.50160878,  2.48798464],\n",
       "       [ 2.49786861,  2.48025871],\n",
       "       [ 2.88192693,  2.88092568],\n",
       "       [ 2.46619742,  2.45529871],\n",
       "       [ 0.42707712,  0.43758266],\n",
       "       [ 3.14067873,  3.13852999],\n",
       "       [ 1.49619062,  1.48979333],\n",
       "       [ 0.40808594,  0.39494217],\n",
       "       [ 0.34156368,  0.35184759],\n",
       "       [ 3.03987122,  2.99106713],\n",
       "       [ 2.79807902,  2.79452837],\n",
       "       [ 0.34700601,  0.33953588],\n",
       "       [ 2.66112011,  2.6478291 ],\n",
       "       [ 1.77276384,  1.75497528],\n",
       "       [ 2.34083553,  2.32635956],\n",
       "       [ 2.0217342 ,  1.98704794],\n",
       "       [ 2.85430777,  2.83695338],\n",
       "       [ 3.06676209,  3.05263527],\n",
       "       [ 3.20708271,  3.18476571],\n",
       "       [ 2.64237432,  2.64195558],\n",
       "       [ 2.79735039,  2.78964679],\n",
       "       [ 2.86203267,  2.84531736],\n",
       "       [ 2.88170762,  2.85040571],\n",
       "       [ 2.68737876,  2.68480673],\n",
       "       [ 3.66379897,  3.61757337],\n",
       "       [ 2.93247241,  2.92496215],\n",
       "       [ 2.68579689,  2.65878432],\n",
       "       [ 1.81150133,  1.80552255],\n",
       "       [ 3.16781601,  3.14807119],\n",
       "       [ 1.96220929,  1.9568402 ],\n",
       "       [ 2.796764  ,  2.7952906 ],\n",
       "       [ 3.37092698,  3.33874223],\n",
       "       [ 0.81100974,  0.8197144 ],\n",
       "       [ 2.4882638 ,  2.48392456],\n",
       "       [ 0.30609168,  0.30439365],\n",
       "       [ 1.70501367,  1.70093097],\n",
       "       [ 0.21852571,  0.24438392],\n",
       "       [ 0.42058972,  0.44470779],\n",
       "       [ 1.71382384,  1.70806927],\n",
       "       [ 0.6915076 ,  0.69249648],\n",
       "       [ 0.40496469,  0.41800678],\n",
       "       [ 2.11586093,  2.10098022],\n",
       "       [ 2.84081896,  2.80270659],\n",
       "       [ 2.91599583,  2.89286319],\n",
       "       [ 2.45114942,  2.43190984],\n",
       "       [ 3.1383909 ,  3.12229467],\n",
       "       [ 2.61852887,  2.60274475],\n",
       "       [ 2.20942255,  2.2086755 ],\n",
       "       [ 2.94809311,  2.94636557],\n",
       "       [ 2.11716827,  2.11339794],\n",
       "       [ 2.96024323,  2.94289891],\n",
       "       [ 0.21097869,  0.20831594],\n",
       "       [ 2.54210127,  2.53891233],\n",
       "       [ 3.24558672,  3.23444128],\n",
       "       [ 3.33611089,  3.31097772],\n",
       "       [ 2.41394794,  2.41141365],\n",
       "       [ 0.16444969,  0.1643932 ],\n",
       "       [ 3.79876246,  3.75535959],\n",
       "       [ 2.39359069,  2.39309925],\n",
       "       [ 2.68228424,  2.66380445],\n",
       "       [ 1.7478795 ,  1.73781651],\n",
       "       [ 2.36181029,  2.35356822],\n",
       "       [ 3.61923088,  3.60387691],\n",
       "       [ 0.12736608,  0.11183737],\n",
       "       [ 0.08661193,  0.09089265],\n",
       "       [ 2.37497986,  2.37214288],\n",
       "       [ 3.79086842,  3.76169232],\n",
       "       [ 0.3459249 ,  0.35051378],\n",
       "       [ 2.14915496,  2.13744783],\n",
       "       [ 0.37147976,  0.3731543 ],\n",
       "       [ 2.65362602,  2.64351948],\n",
       "       [ 2.00488641,  1.99844565],\n",
       "       [ 0.20065129,  0.19149258],\n",
       "       [ 3.1003115 ,  3.0626256 ],\n",
       "       [ 3.1918927 ,  3.16493087],\n",
       "       [ 2.62885355,  2.61654916],\n",
       "       [ 1.59994755,  1.59385834],\n",
       "       [ 2.31737712,  2.31154467],\n",
       "       [ 0.43758954,  0.45696187],\n",
       "       [ 3.42550033,  3.40124175],\n",
       "       [ 0.14174842,  0.14017257],\n",
       "       [ 2.5355274 ,  2.52728523],\n",
       "       [ 1.41231255,  1.39778422],\n",
       "       [ 3.47088083,  3.4403359 ],\n",
       "       [ 2.13222998,  2.11708977],\n",
       "       [ 0.4498278 ,  0.48443678],\n",
       "       [ 2.72413567,  2.68583291],\n",
       "       [ 1.51983892,  1.50999655],\n",
       "       [ 1.40089852,  1.37870202],\n",
       "       [ 2.90887059,  2.89195957],\n",
       "       [ 2.88436555,  2.8744233 ],\n",
       "       [ 0.32586512,  0.33435581],\n",
       "       [ 2.95546816,  2.94874282],\n",
       "       [ 0.44842679,  0.46116477],\n",
       "       [ 3.03577931,  3.02722787],\n",
       "       [ 2.53069793,  2.51937189],\n",
       "       [ 3.26024969,  3.25675639],\n",
       "       [ 2.95031579,  2.93313895],\n",
       "       [ 2.53365032,  2.51955934],\n",
       "       [ 3.24063595,  3.23604509],\n",
       "       [ 0.25845388,  0.2591938 ],\n",
       "       [ 0.97115565,  0.96667363],\n",
       "       [ 1.80684713,  1.80628124],\n",
       "       [ 3.12481438,  3.1011881 ],\n",
       "       [ 1.3414244 ,  1.33960507],\n",
       "       [ 1.47430778,  1.47414279],\n",
       "       [ 2.85392517,  2.84165393],\n",
       "       [ 1.5826954 ,  1.57650019],\n",
       "       [ 2.97015961,  2.95733975],\n",
       "       [ 2.24559925,  2.22334746],\n",
       "       [ 2.97749279,  2.97304165],\n",
       "       [ 0.2955064 ,  0.37861203],\n",
       "       [ 2.47910042,  2.46476422],\n",
       "       [ 2.17066237,  2.17034588],\n",
       "       [ 3.42525914,  3.39634825],\n",
       "       [ 0.3067755 ,  0.31983442],\n",
       "       [ 2.25758385,  2.23871403],\n",
       "       [ 1.25950227,  1.25935803],\n",
       "       [ 1.72758373,  1.71906074],\n",
       "       [ 2.05551408,  2.04239772],\n",
       "       [ 1.52491995,  1.52476493],\n",
       "       [ 2.91182173,  2.90463755],\n",
       "       [ 1.38918421,  1.38943626],\n",
       "       [ 0.22650913,  0.22652714],\n",
       "       [ 2.73476121,  2.72389794],\n",
       "       [ 2.82579037,  2.82518234],\n",
       "       [ 1.95895375,  1.945131  ],\n",
       "       [ 1.76662241,  1.74358328],\n",
       "       [ 2.59767294,  2.5950806 ],\n",
       "       [ 3.50672088,  3.46952767],\n",
       "       [ 0.27931222,  0.28304305],\n",
       "       [ 2.81295246,  2.7931051 ],\n",
       "       [ 2.58915449,  2.58065072],\n",
       "       [ 3.52662318,  3.51868953],\n",
       "       [ 1.85728094,  1.84937835],\n",
       "       [ 1.26856601,  1.23978834],\n",
       "       [ 0.30945843,  0.33526127],\n",
       "       [ 2.54226963,  2.5364943 ],\n",
       "       [ 2.9975732 ,  2.99199988],\n",
       "       [ 2.74930842,  2.73225423],\n",
       "       [ 2.31500125,  2.28723098],\n",
       "       [ 2.1247942 ,  2.10062018],\n",
       "       [ 2.37496897,  2.35747417],\n",
       "       [ 3.22510932,  3.22121199],\n",
       "       [ 0.31769121,  0.33951531],\n",
       "       [ 2.48135706,  2.46870575],\n",
       "       [ 2.98805386,  2.98535221],\n",
       "       [ 0.36078572,  0.36217979],\n",
       "       [ 3.12197126,  3.11797691],\n",
       "       [ 1.6907473 ,  1.67609752],\n",
       "       [ 1.99059948,  1.98703066],\n",
       "       [ 3.03587754,  3.03110199],\n",
       "       [ 3.12289805,  3.10894678],\n",
       "       [ 3.05334054,  3.02741715],\n",
       "       [ 2.80577263,  2.80070294],\n",
       "       [ 1.15462477,  1.14947536],\n",
       "       [ 0.41766649,  0.42423938],\n",
       "       [ 1.93243449,  1.92877689],\n",
       "       [ 0.21537594,  0.21223829],\n",
       "       [ 2.77941979,  2.77729186],\n",
       "       [ 3.00218245,  2.97339411],\n",
       "       [ 0.90985383,  0.90288093],\n",
       "       [ 0.31054964,  0.29335917],\n",
       "       [ 1.15206293,  1.14360989],\n",
       "       [ 2.43302299,  2.42829308],\n",
       "       [ 1.06115903,  1.05991569],\n",
       "       [ 0.35735022,  0.35827   ],\n",
       "       [ 0.28310031,  0.31021281],\n",
       "       [ 0.16600507,  0.17583918],\n",
       "       [ 0.18781136,  0.18640346],\n",
       "       [ 2.59859477,  2.57775277],\n",
       "       [ 3.17711432,  3.16559948],\n",
       "       [ 3.13338317,  3.12812892],\n",
       "       [ 1.1077724 ,  1.12293644],\n",
       "       [ 2.94869906,  2.94638394],\n",
       "       [ 2.77235601,  2.75781056],\n",
       "       [ 1.7913942 ,  1.77255738],\n",
       "       [ 2.18876661,  2.18730694],\n",
       "       [ 0.64624116,  0.65658191],\n",
       "       [ 3.22437189,  3.21224615],\n",
       "       [ 2.71719845,  2.70176661],\n",
       "       [ 3.23510404,  3.22434169],\n",
       "       [ 0.32134305,  0.32733109],\n",
       "       [ 1.47644666,  1.45392931],\n",
       "       [ 3.0220115 ,  3.00975149],\n",
       "       [ 0.92249558,  0.91117929],\n",
       "       [ 2.98322435,  2.96353876],\n",
       "       [ 3.25868286,  3.24876643],\n",
       "       [ 0.23377394,  0.2366718 ],\n",
       "       [ 2.68673759,  2.68109317],\n",
       "       [ 2.89390934,  2.88577265],\n",
       "       [ 2.52097451,  2.51572821],\n",
       "       [ 0.49402271,  0.5654674 ],\n",
       "       [ 0.43008352,  0.43847346]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(proba_k_0_n_3[0]*10000)\n",
    "proba_k_0_n_3[differs_from_correct_k_0_n_3 == 1]*10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.13625166  1.08943289]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.04110569,  1.02916049],\n",
       "       [ 3.80738262,  3.78849089],\n",
       "       [ 1.21005774,  1.21382827],\n",
       "       ..., \n",
       "       [ 2.89301008,  2.89666951],\n",
       "       [ 1.10998774,  1.11748916],\n",
       "       [ 1.64232461,  1.65174135]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(proba_k_1_n_2[0]*10000)\n",
    "proba_k_1_n_2[differs_from_correct_k_1_n_2 == 1]*10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a model that encapsulate 2 models with different k and n values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiModel():\n",
    "    def __init__(self, k1, n1, k2, n2):\n",
    "        self.model1 = K_skip_n_gram_probability_model(k_value=k1, n_value=n1)\n",
    "        self.model2 = K_skip_n_gram_probability_model(k_value=k2, n_value=n2)\n",
    "    def predict(self, reviews):\n",
    "        proba_result = self.predict_proba(reviews)\n",
    "        pred_result = map(lambda x: 1 if x[0] > x[1] else 0, proba_result)\n",
    "        return np.array(list(pred_result))\n",
    "            \n",
    "    def predict_proba(self, reviews):\n",
    "        result1 = map(functools.partial(get_proba_prediction, self=self.model1), enumerate(reviews))\n",
    "        result1 = sorted(result1, key=lambda x: x[0], reverse=False)\n",
    "        \n",
    "        result2 = map(functools.partial(get_proba_prediction, self=self.model2), enumerate(reviews))\n",
    "        result2 = sorted(result2, key=lambda x: x[0], reverse=False)\n",
    "        \n",
    "        return np.array(list(map(lambda x: (x[1], x[2]), result1))) + np.array(list(map(lambda x: (x[1], x[2]), result2)))\n",
    "    \n",
    "    def fit(self, reviews, labels):\n",
    "        self.model1.fit(reviews, labels)\n",
    "        self.model2.fit(reviews, labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bimodel1 = BiModel(k1=1, n1=2, k2=0, n2=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data result: 0.9816\n",
      "Test data result: 0.85492\n"
     ]
    }
   ],
   "source": [
    "bimodel1.fit(X_train, y_train)\n",
    "print(\"Train data result:\", accuracy_score(y_train, bimodel1.predict(X_train)))\n",
    "print(\"Test data result:\", accuracy_score(y_test, bimodel1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bimodel2 = BiModel(k1=1, n1=2, k2=0, n2=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data result: 0.9784\n",
      "Test data result: 0.85984\n"
     ]
    }
   ],
   "source": [
    "bimodel2.fit(X_train, y_train)\n",
    "print(\"Train data result:\", accuracy_score(y_train, bimodel2.predict(X_train)))\n",
    "print(\"Test data result:\", accuracy_score(y_test, bimodel2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try to remove 100 the most popular words, may be result will become better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)\n",
    "max_review_length = 150\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "X_train = np.array([list(filter(lambda x: x > 100, review)) for review in X_train])\n",
    "X_test = np.array([list(filter(lambda x: x > 100, review)) for review in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data result: 0.99372\n",
      "Test data result: 0.75916\n"
     ]
    }
   ],
   "source": [
    "bimodel1 = BiModel(k1=1, n1=2, k2=0, n2=3)\n",
    "bimodel1.fit(X_train, y_train)\n",
    "print(\"Train data result:\", accuracy_score(y_train, bimodel1.predict(X_train)))\n",
    "print(\"Test data result:\", accuracy_score(y_test, bimodel1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data result: 0.99664\n",
      "Test data result: 0.82828\n"
     ]
    }
   ],
   "source": [
    "bimodel2 = BiModel(k1=1, n1=2, k2=0, n2=2)\n",
    "bimodel2.fit(X_train, y_train)\n",
    "print(\"Train data result:\", accuracy_score(y_train, bimodel2.predict(X_train)))\n",
    "print(\"Test data result:\", accuracy_score(y_test, bimodel2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about removing only 10 the most popular words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)\n",
    "max_review_length = 150\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "X_train = np.array([list(filter(lambda x: x > 10, review)) for review in X_train])\n",
    "X_test = np.array([list(filter(lambda x: x > 10, review)) for review in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data result: 0.99388\n",
      "Test data result: 0.85984\n"
     ]
    }
   ],
   "source": [
    "bimodel1 = BiModel(k1=1, n1=2, k2=0, n2=3)\n",
    "bimodel1.fit(X_train, y_train)\n",
    "print(\"Train data result:\", accuracy_score(y_train, bimodel1.predict(X_train)))\n",
    "print(\"Test data result:\", accuracy_score(y_test, bimodel1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data result: 0.98656\n",
      "Test data result: 0.85804\n",
      "For train:\n",
      "TN = 12214, FP = 286, FN = 50, TP = 12450\n",
      "For test:\n",
      "TN = 9670, FP = 2830, FN = 719, TP = 11781\n"
     ]
    }
   ],
   "source": [
    "bimodel2 = BiModel(k1=1, n1=2, k2=0, n2=2)\n",
    "bimodel2.fit(X_train, y_train)\n",
    "pred1 = bimodel2.predict(X_train)\n",
    "pred2 = bimodel2.predict(X_test)\n",
    "print(\"Train data result:\", accuracy_score(y_train, pred1))\n",
    "print(\"Test data result:\", accuracy_score(y_test, pred2))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, pred1).ravel()\n",
    "print(\"For train:\")\n",
    "print(\"TN = {}, FP = {}, FN = {}, TP = {}\".format(tn, fp, fn, tp))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred2).ravel()\n",
    "print(\"For test:\")\n",
    "print(\"TN = {}, FP = {}, FN = {}, TP = {}\".format(tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, the best result for the training data is 0.99388 and for the test 0.85984."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just to release memory\n",
    "POOL.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
